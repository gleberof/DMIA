{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.longdouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power_sum(l, r, p=1.0):\n",
    "\t\"\"\"\n",
    "\t\tinput: l, r - integers, p - float\n",
    "\t\treturns sum of p powers of integers from [l, r]\n",
    "\t\"\"\"\n",
    "\treturn np.sum(np.power(range(l,r+1), p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_equation(a, b, c):\n",
    "\t\"\"\"\n",
    "\t\tinput: a, b, c - integers\n",
    "\t\treturns float solutions x of the following equation: a x ** 2 + b x + c == 0\n",
    "\t\t\tIn case of two diffrent solution returns tuple / list (x1, x2)\n",
    "\t\t\tIn case of one solution returns one float\n",
    "\t\t\tIn case of no float solutions return None \n",
    "\t\t\tIn case of infinity number of solutions returns 'inf'\n",
    "\t\"\"\"\n",
    "\tif c == 0 and  b == 0 and  a == 0:\n",
    "\t\treturn float('inf')\n",
    "\n",
    "\td= b**2 - 4*a*c\n",
    "\n",
    "\tif d < 0:\n",
    "\t\treturn None\n",
    "\n",
    "\tif d == 0:\n",
    "\t\treturn 1.*(-b+d**.5)/(2*a)\n",
    "\telse:\n",
    "\t\treturn (1.*(-b+d**.5)/(2*a), 1.*(-b-d**.5)/(2*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = 4,4,2\n",
    "x=solve_equation(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x is not None:\n",
    "    if type(x)!=float:\n",
    "        assert a*x[0]**2 + b * x[0] + c == 0\n",
    "        assert a*x[1]**2 + b * x[1] + c == 0\n",
    "    else:\n",
    "        if not np.isinf(x):\n",
    "            assert a*x**2 + b * x + c == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outliers(x, std_mul=3.0):\n",
    "    \"\"\"\n",
    "        input: x - numpy vector, std_mul - positive float\n",
    "        returns copy of x with all outliers (elements, which are beyond std_mul * (standart deviation) from mean)\n",
    "        replaced with mean  \n",
    "    \"\"\"\n",
    "    mean = np.mean(x)\n",
    "    dev = np.std(x)*std_mul\n",
    "    l_bound, u_bound = mean-dev, mean+dev\n",
    "    print(l_bound, u_bound)\n",
    "    res = np.array(x)\n",
    "    res[res > u_bound] = mean\n",
    "    res[res < l_bound] = mean\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_eigenvector(A, alpha):\n",
    "        \"\"\"\n",
    "            input: A - square numpy matrix, alpha - float\n",
    "            returns numpy vector - any eigenvector of A corresponding to eigenvalue alpha, \n",
    "                    or None if alpha is not an eigenvalue.\n",
    "        \"\"\"\n",
    "        ws, vs = np.linalg.eig(A)\n",
    "        if alpha in ws:\n",
    "            i, = np.where(ws ==alpha)[0]\n",
    "            return vs[i]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.diag([1,2,3])\n",
    "alpha = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print (get_eigenvector(A, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_sampler(p):\n",
    "    \"\"\"\n",
    "        input: p - numpy vector of probability (non-negative, sums to 1)\n",
    "        returns integer from 0 to len(p) - 1, each integer i is returned with probability p[i] \n",
    "    \"\"\"\n",
    "    select = np.random.random()\n",
    "    p_new = np.array([ np.sum(p[:i+1]) for i in range(len(p))])\n",
    "    i, =  np.where(p_new > select)\n",
    "    return i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print (discrete_sampler([0.1, 0.7, 0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(x, mu=0.0, sigma=1.0):\n",
    "    \"\"\"\n",
    "        input: x - numpy vector, mu - float, sigma - positive float\n",
    "        returns log p(x| mu, sigma) - log-likelihood of x dataset \n",
    "        in univariate gaussian model with mean mu and standart deviation sigma\n",
    "    \"\"\"\n",
    "\n",
    "    return -.5*len(x)*np.log(2*np.pi*sigma**2)-.5*np.sum((x-mu)**2)/(sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.array([7,6,10,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 97 Âµs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-168.1757541328187"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gaussian_log_likelihood(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_approx(f, x0, eps=1e-8):\n",
    "    \"\"\"\n",
    "        input: f - callable, function of vector x. x0 - numpy vector, eps - float, represents step for x_i\n",
    "        returns numpy vector - gradient of f in x0 calculated with finite difference method \n",
    "        (for reference use https://en.wikipedia.org/wiki/Numerical_differentiation, search for \"first-order divided difference\")\n",
    "    \"\"\"\n",
    "    ln = x.shape[0]\n",
    "    xh = np.zeros((ln,ln))\n",
    "    np.fill_diagonal(xh, eps)\n",
    "    xh = x + xh\n",
    "    return (np.apply_along_axis(f, axis = 1, arr = xh) - np.apply_along_axis(f, axis = 1, arr = np.tile(x, (ln,1))))/eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_method(f, x0, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "    \"\"\"\n",
    "        input: f - function of x. x0 - numpy vector, n_steps - integer, learning rate, eps - float.\n",
    "        returns tuple (f^*, x^*), where x^* is local minimum point, found after n_steps of gradient descent, \n",
    "                                        f^* - resulting function value.\n",
    "        Impletent gradient descent method, given in the lecture. \n",
    "        For gradient use finite difference approximation with eps step.\n",
    "    \"\"\"\n",
    "    for i in range(steps):\n",
    "        x0 -= learning_rate* gradient_approx(f, x0, eps=eps)\n",
    "    return (f(x0), x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_predict(w, b, X):\n",
    "    \"\"\"\n",
    "        input: w - numpy vector of M weights, b - bias, X - numpy matrix N x M (object-feature matrix), \n",
    "        N - number of objects, M - number of features.\n",
    "        returns numpy vector of predictions of linear regression model for X\n",
    "        https://xkcd.com/1725/\n",
    "    \"\"\"\n",
    "    return np.sum(X*w, axis=1) + np.tile(b, X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        input: two numpy vectors of object targets and model predictions.\n",
    "        return mse\n",
    "    \"\"\"\n",
    "    return np.mean((y_true-y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_mse_gradient(w, b, X, y_true):\n",
    "    \"\"\"\n",
    "        input: w, b - weights and bias of a linear regression model,\n",
    "                X - object-feature matrix, y_true - targets.\n",
    "        returns gradient of linear regression model mean squared error w.r.t w and b\n",
    "    \"\"\"\n",
    "    ln = X.shape[0]\n",
    "    wt = np.array(np.append(w, b), dtype=np.float64)\n",
    "    X0 = np.array(np.hstack((X, np.ones((ln, 1)))), dtype=np.float64)\n",
    "    res = 2. * X0.T.dot(X0.dot(wt)-y_true) / ln\n",
    "    return res[:-1], res[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegressor:\n",
    "    def fit(self, X_train, y_train, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "        \"\"\"\n",
    "            input: object-feature matrix and targets.\n",
    "            optimises mse w.r.t model parameters \n",
    "        \"\"\"\n",
    "        self.w = np.zeros(X_train.shape[1], dtype=np.float64)\n",
    "        self.b = 0.0\n",
    "        mse = mean_squared_error(y_train, linear_regression_predict(self.w, self.b, X_train))\n",
    "        self.mse = np.array(mse, dtype=np.float64)\n",
    "        for i in range(n_steps):\n",
    "            g_w, g_b = linear_regression_mse_gradient(self.w, self.b, X_train, y_train)\n",
    "            print(i, g_w,\"\\n\" ,g_b)\n",
    "            self.w -= g_w * learning_rate\n",
    "            self.b -= g_b * learning_rate\n",
    "            print(self.w, \"\\n\", self.b)\n",
    "            \n",
    "            new_mse = mean_squared_error(y_train, linear_regression_predict(self.w, self.b, X_train))\n",
    "            \n",
    "            self.mse = np.append(self.mse, new_mse)\n",
    "            if abs(new_mse - mse) < eps:\n",
    "                mse=new_mse\n",
    "                break\n",
    "            else:\n",
    "                mse = new_mse\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return linear_regression_predict(self.w, self.b, X)\n",
    "    \n",
    "    def get_mse(self):\n",
    "        return self.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_der(x):\n",
    "    \"\"\"\n",
    "        returns sigmoid derivative w.r.t. x\n",
    "    \"\"\"\n",
    "    sig = sigmoid(x)\n",
    "    return sig*(1-sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_der(x):\n",
    "    \"\"\"\n",
    "        return relu (sub-)derivative w.r.t x\n",
    "    \"\"\"\n",
    "    return 0 if x <=0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressorMLP:\n",
    "    def __init__():\n",
    "        self.w = np.random.rand(X_train.shape[1]) - 0.5\n",
    "        self.b = mp.random.random()  - 0.5\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "        \"\"\"\n",
    "            input: object-feature matrix and targets.\n",
    "            optimises mse w.r.t model parameters \n",
    "        \"\"\"\n",
    "\n",
    "        mse = mean_squared_error(y_train, linear_regression_predict(self.w, self.b, X_train))\n",
    "        for i in range(n_steps):\n",
    "            g_w, g_b = linear_regression_mse_gradient(self.w, self.b, X_train, y_train)\n",
    "            self.w -= g_w * learning_rate\n",
    "            self.b -= g_b * learning_rate\n",
    "            \n",
    "            new_mse = mean_squared_error(y_train, linear_regression_predict(self.w, self.b, X_train))\n",
    "            \n",
    "            if abs(new_mse - mse) < eps:\n",
    "                mse=new_mse\n",
    "                break\n",
    "            else:\n",
    "                mse = new_mse\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return linear_regression_predict(self.w, self.b, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(4,-1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor:\n",
    "    \"\"\"\n",
    "        simple dense neural network class for regression with mse loss. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_units=[32, 32], nonlinearity=relu):\n",
    "        \"\"\"\n",
    "            input: n_units - number of neurons for each hidden layer in neural network,\n",
    "                   nonlinearity - activation function applied between hidden layers.\n",
    "        \"\"\"\n",
    "        self.n_units = n_units\n",
    "        self.nonlinearity = nonlinearity\n",
    "        \n",
    "        # set neurons\n",
    "        self.h_layers = list()\n",
    "        for i, k in enumerate(self.n_units):\n",
    "            self.h_layers.append(list())\n",
    "            for n in range(k):\n",
    "                mlp_lr = LinearRegressorMLP()\n",
    "                self.h_layers[i].append(mlp_lr)\n",
    "        \n",
    "        # set output layer\n",
    "        self.output = LinearRegressorMLP()\n",
    "        self.layer_input = np.array()\n",
    "        self.input_for_output = np.array()\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, n_steps=1000, learning_rate=1e-2, eps=1e-8):\n",
    "        \"\"\"\n",
    "            input: object-feature matrix and targets.\n",
    "            optimises mse w.r.t model parameters\n",
    "            (you may use approximate gradient estimation)\n",
    "        \"\"\"\n",
    "        \n",
    "        mse = mean_squared_error(y_train, self.predict(X_train)) \n",
    "        for i in range(n_steps):\n",
    "            #forward\n",
    "            y_pred = self.predict(X_train)\n",
    "            #backwardpropagation\n",
    "            \n",
    "            # output calculation\n",
    "            sig_outp = (y_train - y_pred)*gradient_approx(self.nonlinearity, self.output_before_relu)\n",
    "            # neurons calculation\n",
    "\n",
    "            #weights update neurons\n",
    "            \n",
    "            #weights update output\n",
    "            \n",
    "            #check stop condition\n",
    "            new_mse = mean_squared_error(y_train, self.predict(X_train)) \n",
    "            if abs(mse-new_mse) < eps:\n",
    "                break\n",
    "            else\n",
    "                mse= new_mse\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            input: object-feature matrix\n",
    "            returns MLP predictions in X\n",
    "        \"\"\"\n",
    "        self.output_wo_relu = np.array()\n",
    "        self.layer_input = np.array()\n",
    "        current_signal = np.array(X)\n",
    "        before_relu = np.array(X)\n",
    "        for lay in self.h_layers:\n",
    "            self.layer_input = np.append(self.layer_input, current_signal)\n",
    "            self.output_wo_relu = np.append(self.output_wo_relu, before_relu)\n",
    "            next_signal = np.array()\n",
    "            before_relu = np.array()\n",
    "            for n in lay:\n",
    "                pred = n.predict(current_signal)\n",
    "                before_relu = np.append(before_relu, pred)\n",
    "                next_input = np.append(next_signal, self.nonlinearity(pred))\n",
    "            current_signal = np_array(next_signal)\n",
    "            \n",
    "            \n",
    "        self.input_for_output = np.arrya(current_signal)\n",
    "        self.output_before_relu = self.output.predict(current-signal)\n",
    "            \n",
    "        return np.array(self.nonlinearity(self.output_before_relu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
